# Local RAG Agent with Ollama

Run `ingest.py` to build the vector store from PDFs in the `docs` folder.
Then run `main.py` to ask questions using a local LLM (Ollama).